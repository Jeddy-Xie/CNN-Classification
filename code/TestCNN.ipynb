{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from CNNModel import CNN\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are three versions of MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTypes = [\"digits-normal.mat\", \"digits-scaled.mat\", \"digits-jitter.mat\"]\n",
    "\n",
    "# Accuracy placeholder\n",
    "accuracy = np.zeros(len(dataTypes))\n",
    "trainSet = 1\n",
    "testSet = 2\n",
    "\n",
    "dataPath = os.path.join(\"/Users/jeddyxie/Desktop/Computer Vision/HW3/Assignment/\", \"data\", \"digits-jitter.mat\")\n",
    "data = utils.loadmat(dataPath)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Overview\n",
    "\n",
    "This notebook includes several key functions used for model training, evaluation, reinitialization, and hyperparameter tuning:\n",
    "\n",
    "- **train(model, dataloader, loss_function, optimizer, epochs, display=True)**  \n",
    "  Trains the model on the training dataset over a specified number of epochs. It performs forward and backward passes, updates the model parameters, computes the loss and accuracy for each epoch, and optionally prints these metrics. The function returns arrays with the epoch-wise loss and accuracy.\n",
    "\n",
    "- **evaluate(model, dataloader, loss_function)**  \n",
    "  Evaluates the model's performance on the test dataset. It computes the average loss and accuracy by running the model in evaluation mode (with gradient calculations disabled) to ensure consistency during inference.\n",
    "\n",
    "- **reset_weights(m)**  \n",
    "  A utility function that reinitializes the weights of a model layer if it supports weight resetting (i.e., has a `reset_parameters` method).\n",
    "\n",
    "- **objective(learning_rate, batch_size, num_epochs)**  \n",
    "  Defines the objective function for Bayesian optimization. It takes hyperparameters as inputs (learning rate, batch size, and number of epochs), casts those that need to be integers, trains the model using these hyperparameters, and evaluates its test accuracy. The test accuracy is returned as the metric to be maximized during the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimizer, epochs, display=True):\n",
    "    model.train()\n",
    "    loss_arr = np.zeros(epochs)\n",
    "    acc_arr = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_pred = 0\n",
    "        total_pred = 0\n",
    "        \n",
    "        for images, labels in dataloader:\n",
    "            # Move the batch to GPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct_pred += (predicted == labels).sum().item()\n",
    "            total_pred += labels.size(0)\n",
    "        \n",
    "        total_epoch_loss = total_loss / total_pred\n",
    "        epoch_accuracy = correct_pred / total_pred\n",
    "        loss_arr[epoch] = total_epoch_loss\n",
    "        acc_arr[epoch] = epoch_accuracy\n",
    "        \n",
    "        if display:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {total_epoch_loss}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "    \n",
    "    return loss_arr, acc_arr\n",
    "\n",
    "def evaluate(model, dataloader, loss_function):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            # Move the batch to GPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct_pred += (predicted == labels).sum().item()\n",
    "            total_pred += labels.size(0)\n",
    "    test_loss /= total_pred\n",
    "    test_acc = correct_pred / total_pred\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def objective(learning_rate, batch_size, num_epochs, xTrain, yTrain, xTest, yTest):\n",
    "    # Convert hyperparameters to appropriate types\n",
    "    batch_size = int(batch_size)\n",
    "    num_epochs = int(num_epochs)\n",
    "    \n",
    "    # Define model, loss function, and optimizer\n",
    "    model = CNN()  # Assuming CNN is defined elsewhere and imported\n",
    "    model.to(device)  # Move model to GPU\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create DataLoaders with drop_last=True to avoid batches with a single sample\n",
    "    train_dataset = TensorDataset(xTrain, yTrain)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    test_dataset = TensorDataset(xTest, yTest)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    # Reset model weights before training\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    # Train the model (display is False for hyperparameter tuning)\n",
    "    train(model, train_loader, loss_function, optimizer, num_epochs, display=False)\n",
    "    \n",
    "    # Evaluate on the test set and return test accuracy\n",
    "    _, test_acc = evaluate(model, test_loader, loss_function)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.25, inplace=False)\n",
      "    (5): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model Check\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Loading dataset: digits-normal.mat ...\n",
      "+++ Loading dataset: digits-normal.mat (2000 images)\n",
      "|   iter    |  target   | batch_... | learni... | num_ep... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9339   \u001b[39m | \u001b[39m67.96    \u001b[39m | \u001b[39m0.009508 \u001b[39m | \u001b[39m86.6     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9461   \u001b[39m | \u001b[35m89.47    \u001b[39m | \u001b[35m0.001569 \u001b[39m | \u001b[35m57.8     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.9543   \u001b[39m | \u001b[35m37.58    \u001b[39m | \u001b[35m0.008663 \u001b[39m | \u001b[35m80.06    \u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.9576   \u001b[39m | \u001b[35m99.97    \u001b[39m | \u001b[35m0.0002156\u001b[39m | \u001b[35m98.5     \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.955    \u001b[39m | \u001b[39m111.9    \u001b[39m | \u001b[39m0.002131 \u001b[39m | \u001b[39m59.09    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9579   \u001b[39m | \u001b[35m101.3    \u001b[39m | \u001b[35m0.002349 \u001b[39m | \u001b[35m98.75    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9283   \u001b[39m | \u001b[39m123.0    \u001b[39m | \u001b[39m0.006521 \u001b[39m | \u001b[39m83.79    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.873    \u001b[39m | \u001b[39m124.4    \u001b[39m | \u001b[39m0.006116 \u001b[39m | \u001b[39m50.03    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9442   \u001b[39m | \u001b[39m103.1    \u001b[39m | \u001b[39m0.004717 \u001b[39m | \u001b[39m65.7     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9406   \u001b[39m | \u001b[39m101.5    \u001b[39m | \u001b[39m0.0056   \u001b[39m | \u001b[39m87.16    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9396   \u001b[39m | \u001b[39m32.25    \u001b[39m | \u001b[39m0.009494 \u001b[39m | \u001b[39m68.46    \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.9677   \u001b[39m | \u001b[35m33.73    \u001b[39m | \u001b[35m0.009165 \u001b[39m | \u001b[35m91.71    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9514   \u001b[39m | \u001b[39m43.92    \u001b[39m | \u001b[39m0.00839  \u001b[39m | \u001b[39m94.02    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9563   \u001b[39m | \u001b[39m32.56    \u001b[39m | \u001b[39m0.001882 \u001b[39m | \u001b[39m99.71    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9437   \u001b[39m | \u001b[39m66.81    \u001b[39m | \u001b[39m0.001968 \u001b[39m | \u001b[39m50.05    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9525   \u001b[39m | \u001b[39m44.83    \u001b[39m | \u001b[39m0.008832 \u001b[39m | \u001b[39m50.13    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.953    \u001b[39m | \u001b[39m52.33    \u001b[39m | \u001b[39m0.004265 \u001b[39m | \u001b[39m65.2     \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9618   \u001b[39m | \u001b[39m83.09    \u001b[39m | \u001b[39m0.006989 \u001b[39m | \u001b[39m99.8     \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9412   \u001b[39m | \u001b[39m85.51    \u001b[39m | \u001b[39m0.005942 \u001b[39m | \u001b[39m89.16    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9282   \u001b[39m | \u001b[39m101.9    \u001b[39m | \u001b[39m0.004751 \u001b[39m | \u001b[39m50.15    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.944    \u001b[39m | \u001b[39m50.6     \u001b[39m | \u001b[39m0.006279 \u001b[39m | \u001b[39m78.51    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.9396   \u001b[39m | \u001b[39m32.11    \u001b[39m | \u001b[39m0.003831 \u001b[39m | \u001b[39m50.16    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.9421   \u001b[39m | \u001b[39m72.35    \u001b[39m | \u001b[39m0.007326 \u001b[39m | \u001b[39m99.87    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.9275   \u001b[39m | \u001b[39m67.39    \u001b[39m | \u001b[39m0.006616 \u001b[39m | \u001b[39m66.08    \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.925    \u001b[39m | \u001b[39m32.33    \u001b[39m | \u001b[39m0.008902 \u001b[39m | \u001b[39m85.28    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.9444   \u001b[39m | \u001b[39m36.2     \u001b[39m | \u001b[39m0.007763 \u001b[39m | \u001b[39m94.79    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.9563   \u001b[39m | \u001b[39m32.1     \u001b[39m | \u001b[39m0.003046 \u001b[39m | \u001b[39m93.85    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.9395   \u001b[39m | \u001b[39m86.97    \u001b[39m | \u001b[39m0.001904 \u001b[39m | \u001b[39m99.95    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.9515   \u001b[39m | \u001b[39m79.64    \u001b[39m | \u001b[39m0.003967 \u001b[39m | \u001b[39m99.59    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.9466   \u001b[39m | \u001b[39m36.37    \u001b[39m | \u001b[39m0.007726 \u001b[39m | \u001b[39m90.12    \u001b[39m |\n",
      "=============================================================\n",
      "Best parameters for digits-normal.mat:\n",
      "{'target': np.float64(0.9676767676767677), 'params': {'batch_size': np.float64(33.72608512863598), 'learning_rate': np.float64(0.009164717474804024), 'num_epochs': np.float64(91.7054875023434)}}\n",
      "+++ Loading dataset: digits-scaled.mat ...\n",
      "+++ Loading dataset: digits-scaled.mat (2000 images)\n",
      "|   iter    |  target   | batch_... | learni... | num_ep... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.919    \u001b[39m | \u001b[39m67.96    \u001b[39m | \u001b[39m0.009508 \u001b[39m | \u001b[39m86.6     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9303   \u001b[39m | \u001b[35m89.47    \u001b[39m | \u001b[35m0.001569 \u001b[39m | \u001b[35m57.8     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.9376   \u001b[39m | \u001b[35m37.58    \u001b[39m | \u001b[35m0.008663 \u001b[39m | \u001b[35m80.06    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9111   \u001b[39m | \u001b[39m99.97    \u001b[39m | \u001b[39m0.0002156\u001b[39m | \u001b[39m98.5     \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9324   \u001b[39m | \u001b[39m111.9    \u001b[39m | \u001b[39m0.002131 \u001b[39m | \u001b[39m59.09    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9396   \u001b[39m | \u001b[35m32.29    \u001b[39m | \u001b[35m0.00254  \u001b[39m | \u001b[35m63.94    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9293   \u001b[39m | \u001b[39m33.46    \u001b[39m | \u001b[39m0.007297 \u001b[39m | \u001b[39m63.65    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9375   \u001b[39m | \u001b[39m32.04    \u001b[39m | \u001b[39m0.00452  \u001b[39m | \u001b[39m64.69    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9211   \u001b[39m | \u001b[39m38.87    \u001b[39m | \u001b[39m0.005299 \u001b[39m | \u001b[39m81.67    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9295   \u001b[39m | \u001b[39m36.42    \u001b[39m | \u001b[39m0.002165 \u001b[39m | \u001b[39m79.11    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9396   \u001b[39m | \u001b[39m32.03    \u001b[39m | \u001b[39m0.007631 \u001b[39m | \u001b[39m62.41    \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.9417   \u001b[39m | \u001b[35m32.43    \u001b[39m | \u001b[35m0.002757 \u001b[39m | \u001b[35m60.5     \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9271   \u001b[39m | \u001b[39m32.03    \u001b[39m | \u001b[39m0.001847 \u001b[39m | \u001b[39m58.99    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9374   \u001b[39m | \u001b[39m33.47    \u001b[39m | \u001b[39m0.005043 \u001b[39m | \u001b[39m61.07    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9167   \u001b[39m | \u001b[39m39.1     \u001b[39m | \u001b[39m0.006975 \u001b[39m | \u001b[39m78.09    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9139   \u001b[39m | \u001b[39m34.76    \u001b[39m | \u001b[39m0.006397 \u001b[39m | \u001b[39m59.21    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9209   \u001b[39m | \u001b[39m36.49    \u001b[39m | \u001b[39m0.007711 \u001b[39m | \u001b[39m80.85    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9062   \u001b[39m | \u001b[39m32.77    \u001b[39m | \u001b[39m0.004622 \u001b[39m | \u001b[39m62.13    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9222   \u001b[39m | \u001b[39m90.79    \u001b[39m | \u001b[39m0.0006245\u001b[39m | \u001b[39m71.24    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9182   \u001b[39m | \u001b[39m88.87    \u001b[39m | \u001b[39m0.007199 \u001b[39m | \u001b[39m56.4     \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.9221   \u001b[39m | \u001b[39m122.8    \u001b[39m | \u001b[39m0.00452  \u001b[39m | \u001b[39m95.51    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.9093   \u001b[39m | \u001b[39m97.34    \u001b[39m | \u001b[39m0.003545 \u001b[39m | \u001b[39m57.21    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.9397   \u001b[39m | \u001b[39m64.29    \u001b[39m | \u001b[39m0.0005907\u001b[39m | \u001b[39m93.93    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.9224   \u001b[39m | \u001b[39m116.5    \u001b[39m | \u001b[39m0.008379 \u001b[39m | \u001b[39m66.04    \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.932    \u001b[39m | \u001b[39m63.59    \u001b[39m | \u001b[39m0.009631 \u001b[39m | \u001b[39m75.26    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.931    \u001b[39m | \u001b[39m105.8    \u001b[39m | \u001b[39m0.0008255\u001b[39m | \u001b[39m81.92    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.9289   \u001b[39m | \u001b[39m82.57    \u001b[39m | \u001b[39m0.00222  \u001b[39m | \u001b[39m96.79    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.9109   \u001b[39m | \u001b[39m38.75    \u001b[39m | \u001b[39m0.007809 \u001b[39m | \u001b[39m93.63    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.928    \u001b[39m | \u001b[39m50.38    \u001b[39m | \u001b[39m0.004212 \u001b[39m | \u001b[39m94.17    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.9275   \u001b[39m | \u001b[39m91.23    \u001b[39m | \u001b[39m0.0004805\u001b[39m | \u001b[39m92.22    \u001b[39m |\n",
      "=============================================================\n",
      "Best parameters for digits-scaled.mat:\n",
      "{'target': np.float64(0.9416666666666667), 'params': {'batch_size': np.float64(32.43326794360671), 'learning_rate': np.float64(0.002756762482682119), 'num_epochs': np.float64(60.49820740947596)}}\n",
      "+++ Loading dataset: digits-jitter.mat ...\n",
      "+++ Loading dataset: digits-jitter.mat (2000 images)\n",
      "|   iter    |  target   | batch_... | learni... | num_ep... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7186   \u001b[39m | \u001b[39m67.96    \u001b[39m | \u001b[39m0.009508 \u001b[39m | \u001b[39m86.6     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.7978   \u001b[39m | \u001b[35m89.47    \u001b[39m | \u001b[35m0.001569 \u001b[39m | \u001b[35m57.8     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.8254   \u001b[39m | \u001b[35m37.58    \u001b[39m | \u001b[35m0.008663 \u001b[39m | \u001b[35m80.06    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.7394   \u001b[39m | \u001b[39m99.97    \u001b[39m | \u001b[39m0.0002156\u001b[39m | \u001b[39m98.5     \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.8221   \u001b[39m | \u001b[39m111.9    \u001b[39m | \u001b[39m0.002131 \u001b[39m | \u001b[39m59.09    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.7737   \u001b[39m | \u001b[39m33.09    \u001b[39m | \u001b[39m0.006968 \u001b[39m | \u001b[39m68.8     \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.808    \u001b[39m | \u001b[39m112.3    \u001b[39m | \u001b[39m0.006177 \u001b[39m | \u001b[39m58.83    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.7545   \u001b[39m | \u001b[39m110.1    \u001b[39m | \u001b[39m0.00967  \u001b[39m | \u001b[39m59.77    \u001b[39m |\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m0.8376   \u001b[39m | \u001b[35m36.89    \u001b[39m | \u001b[35m0.007212 \u001b[39m | \u001b[35m80.85    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.8004   \u001b[39m | \u001b[39m37.99    \u001b[39m | \u001b[39m0.009074 \u001b[39m | \u001b[39m81.32    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.8184   \u001b[39m | \u001b[39m35.97    \u001b[39m | \u001b[39m0.006828 \u001b[39m | \u001b[39m79.72    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.8204   \u001b[39m | \u001b[39m35.61    \u001b[39m | \u001b[39m0.001385 \u001b[39m | \u001b[39m81.4     \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.8208   \u001b[39m | \u001b[39m113.3    \u001b[39m | \u001b[39m0.005427 \u001b[39m | \u001b[39m60.58    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.818    \u001b[39m | \u001b[39m114.4    \u001b[39m | \u001b[39m0.006479 \u001b[39m | \u001b[39m62.28    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.813    \u001b[39m | \u001b[39m115.6    \u001b[39m | \u001b[39m0.00501  \u001b[39m | \u001b[39m60.25    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.6964   \u001b[39m | \u001b[39m112.2    \u001b[39m | \u001b[39m0.008069 \u001b[39m | \u001b[39m62.49    \u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m0.8427   \u001b[39m | \u001b[35m116.3    \u001b[39m | \u001b[35m0.004225 \u001b[39m | \u001b[35m62.5     \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.7155   \u001b[39m | \u001b[39m116.1    \u001b[39m | \u001b[39m0.003848 \u001b[39m | \u001b[39m64.02    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.7739   \u001b[39m | \u001b[39m115.9    \u001b[39m | \u001b[39m0.00999  \u001b[39m | \u001b[39m61.64    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.778    \u001b[39m | \u001b[39m117.0    \u001b[39m | \u001b[39m0.003605 \u001b[39m | \u001b[39m62.67    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8135   \u001b[39m | \u001b[39m122.8    \u001b[39m | \u001b[39m0.00452  \u001b[39m | \u001b[39m95.51    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7807   \u001b[39m | \u001b[39m122.8    \u001b[39m | \u001b[39m0.002738 \u001b[39m | \u001b[39m95.47    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.7946   \u001b[39m | \u001b[39m64.29    \u001b[39m | \u001b[39m0.0005907\u001b[39m | \u001b[39m93.93    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.7629   \u001b[39m | \u001b[39m116.5    \u001b[39m | \u001b[39m0.008379 \u001b[39m | \u001b[39m66.04    \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.7914   \u001b[39m | \u001b[39m63.59    \u001b[39m | \u001b[39m0.009631 \u001b[39m | \u001b[39m75.26    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.7833   \u001b[39m | \u001b[39m105.8    \u001b[39m | \u001b[39m0.0008255\u001b[39m | \u001b[39m81.92    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.8028   \u001b[39m | \u001b[39m82.57    \u001b[39m | \u001b[39m0.00222  \u001b[39m | \u001b[39m96.79    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8198   \u001b[39m | \u001b[39m38.75    \u001b[39m | \u001b[39m0.007809 \u001b[39m | \u001b[39m93.63    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.78     \u001b[39m | \u001b[39m50.38    \u001b[39m | \u001b[39m0.004212 \u001b[39m | \u001b[39m94.17    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.7604   \u001b[39m | \u001b[39m91.23    \u001b[39m | \u001b[39m0.0004805\u001b[39m | \u001b[39m92.22    \u001b[39m |\n",
      "=============================================================\n",
      "Best parameters for digits-jitter.mat:\n",
      "{'target': np.float64(0.8426724137931034), 'params': {'batch_size': np.float64(116.33950740811109), 'learning_rate': np.float64(0.004225360329435581), 'num_epochs': np.float64(62.49557717341271)}}\n",
      "All best hyperparameters:\n",
      "{'digits-normal.mat': {'target': np.float64(0.9676767676767677), 'params': {'batch_size': np.float64(33.72608512863598), 'learning_rate': np.float64(0.009164717474804024), 'num_epochs': np.float64(91.7054875023434)}}, 'digits-scaled.mat': {'target': np.float64(0.9416666666666667), 'params': {'batch_size': np.float64(32.43326794360671), 'learning_rate': np.float64(0.002756762482682119), 'num_epochs': np.float64(60.49820740947596)}}, 'digits-jitter.mat': {'target': np.float64(0.8426724137931034), 'params': {'batch_size': np.float64(116.33950740811109), 'learning_rate': np.float64(0.004225360329435581), 'num_epochs': np.float64(62.49557717341271)}}}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the best hyperparameters for each dataset\n",
    "best_hyperparams = {}\n",
    "\n",
    "dataPath = os.path.join(\"/Users/jeddyxie/Desktop/Computer Vision/HW3/Assignment/\")\n",
    "\n",
    "# Loop over each dataset in dataTypes\n",
    "for dataType in dataTypes:\n",
    "    print(\"+++ Loading dataset: {} ...\".format(dataType))\n",
    "    path = os.path.join(dataPath, \"data\", dataType)\n",
    "    data = utils.loadmat(path)\n",
    "    print(\"+++ Loading dataset: {} ({} images)\".format(dataType, data[\"x\"].shape[2]))\n",
    "    \n",
    "    # Preprocess the data\n",
    "    x = data[\"x\"].transpose([2, 0, 1])\n",
    "    x = np.reshape(x, [x.shape[0], 1, x.shape[1], x.shape[2]])\n",
    "    y = data[\"y\"]\n",
    "\n",
    "    x = torch.tensor(x).float()\n",
    "    y = torch.tensor(y).long()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    xTrain = x[data[\"set\"] == trainSet, :, :, :]\n",
    "    yTrain = y[data[\"set\"] == trainSet]\n",
    "    xTest = x[data[\"set\"] == testSet, :, :, :]\n",
    "    yTest = y[data[\"set\"] == testSet]\n",
    "    \n",
    "    # Define the bounds for hyperparameters\n",
    "    pbounds = {\n",
    "        'learning_rate': (1e-5, 1e-2),\n",
    "        'batch_size': (32, 128),\n",
    "        'num_epochs': (50, 100)\n",
    "    }\n",
    "    \n",
    "    # Set up Bayesian Optimization.\n",
    "    # The lambda ensures that the extra parameters (xTrain, yTrain, etc.) are passed to the objective.\n",
    "    optimizer_bo = BayesianOptimization(\n",
    "        f=lambda learning_rate, batch_size, num_epochs: objective(learning_rate, batch_size, num_epochs, xTrain, yTrain, xTest, yTest),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    # Run the optimization process\n",
    "    optimizer_bo.maximize(\n",
    "        init_points=5,\n",
    "        n_iter=25\n",
    "    )\n",
    "    \n",
    "    # Store the best hyperparameters for this dataset in the dictionary\n",
    "    best_hyperparams[dataType] = optimizer_bo.max\n",
    "    print(\"Best parameters for {}:\".format(dataType))\n",
    "    print(optimizer_bo.max)\n",
    "    \n",
    "print(\"All best hyperparameters:\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Loading dataset: digits-normal.mat (2000 images)\n",
      "Best hyperparameters for digits-normal.mat:\n",
      "{'batch_size': 33, 'learning_rate': np.float64(0.009164717474804024), 'num_epochs': 91}\n"
     ]
    }
   ],
   "source": [
    "testSet=3\n",
    "dataPath = os.path.join(\"/Users/jeddyxie/Desktop/Computer Vision/HW3/Assignment/\")\n",
    "accuracy = np.zeros(len(dataTypes))\n",
    "\n",
    "for dataset_idx, dataType in enumerate(dataTypes):\n",
    "    # Load data\n",
    "    path = os.path.join(dataPath, \"data\", dataType)\n",
    "    data = utils.loadmat(path)\n",
    "    print(\"+++ Loading dataset: {} ({} images)\".format(dataType, data[\"x\"].shape[2]))\n",
    "    \n",
    "    x = data[\"x\"].transpose([2, 0, 1])\n",
    "    x = np.reshape(x, [x.shape[0], 1, x.shape[1], x.shape[2]])\n",
    "    y = data[\"y\"]\n",
    "    \n",
    "    x = torch.tensor(x).float()\n",
    "    y = torch.tensor(y).long()\n",
    "    \n",
    "    xTrain = x[data[\"set\"] == trainSet, :, :, :]\n",
    "    yTrain = y[data[\"set\"] == trainSet]\n",
    "    xTest = x[data[\"set\"] == testSet, :, :, :]\n",
    "    yTest = y[data[\"set\"] == testSet]\n",
    "    \n",
    "    # Retrieve hyperparameters for the current dataset from best_hyperparams dict.\n",
    "    # If not found, use fallback defaults.\n",
    "    if dataType in best_hyperparams:\n",
    "        print(\"Best hyperparameters for {}:\".format(dataType))\n",
    "        best_params = best_hyperparams[dataType][\"params\"]\n",
    "        hyperparams = {\n",
    "            \"batch_size\": int(best_params[\"batch_size\"]),\n",
    "            \"learning_rate\": best_params[\"learning_rate\"],\n",
    "            \"num_epochs\": int(best_params[\"num_epochs\"])\n",
    "        }\n",
    "        print(hyperparams)\n",
    "    else:\n",
    "        hyperparams = {\n",
    "            \"batch_size\": 64,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"num_epochs\": 150\n",
    "        }\n",
    "    \n",
    "    # Define model, loss function, and optimizer using the hyperparameters.\n",
    "    model = CNN()\n",
    "    model.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "    \n",
    "    # Create train dataset and loader.\n",
    "    train_dataset = TensorDataset(xTrain, yTrain)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Create test dataset and loader.\n",
    "    test_dataset = TensorDataset(xTest, yTest)\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Reset model weights before training.\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    EPOCHS = hyperparams[\"num_epochs\"]\n",
    "    loss, acc = train(model, train_loader, loss_function, optimizer, EPOCHS, display=False)\n",
    "    \n",
    "    # Plot training loss and accuracy.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, EPOCHS + 1), loss, marker='o', linestyle='-', color='blue', label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, EPOCHS + 1), acc, marker='o', linestyle='-', color='green', label='Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to a file named according to the dataset.\n",
    "    plot_filename = f\"training_plots_{dataType}.png\"\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot to {plot_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set.\n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_function)\n",
    "    accuracy[dataset_idx] = test_acc\n",
    "    print('Test accuracy for {}: {:.4f}'.format(dataType, test_acc))\n",
    "\n",
    "print(\"Overall Test Accuracies:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Accuracy Table [trainSet=1, testSet=3]\n",
      "--------------------------------------------------\n",
      "dataset\t\t\tcnn\t\n",
      "--------------------------------------------------\n",
      "digits-normal.mat\t94.40\t\n",
      "digits-scaled.mat\t93.60\t\n",
      "digits-jitter.mat\t75.40\t\n",
      "\n",
      "Hyperparameters for each dataset:\n",
      "=================================\n",
      "\n",
      "Dataset: digits-normal.mat\n",
      "Batch Size: 34\n",
      "Learning Rate: 0.0092\n",
      "Number of Epochs: 92\n",
      "Best Accuracy: 0.9677\n",
      "------------------------------\n",
      "\n",
      "Dataset: digits-scaled.mat\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.0028\n",
      "Number of Epochs: 60\n",
      "Best Accuracy: 0.9417\n",
      "------------------------------\n",
      "\n",
      "Dataset: digits-jitter.mat\n",
      "Batch Size: 116\n",
      "Learning Rate: 0.0042\n",
      "Number of Epochs: 62\n",
      "Best Accuracy: 0.8427\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the results in a table\n",
    "print(\"+++ Accuracy Table [trainSet={}, testSet={}]\".format(trainSet, testSet))\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"dataset\\t\\t\\t\", end=\"\")\n",
    "print(\"{}\\t\".format(\"cnn\"), end=\"\")\n",
    "print()\n",
    "print(\"--------------------------------------------------\")\n",
    "for dataset_idx in range(len(dataTypes)):\n",
    "    print(\"{}\\t\".format(dataTypes[dataset_idx]), end=\"\")\n",
    "    print(\"{:.2f}\\t\".format(accuracy[dataset_idx] * 100))\n",
    "# Print hyperparameters associated with datasets\n",
    "print(\"\\nHyperparameters for each dataset:\")\n",
    "print(\"=================================\")\n",
    "for dataset, results in best_hyperparams.items():\n",
    "    params = results['params']\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    print(f\"Batch Size: {round(params['batch_size'])}\")\n",
    "    print(f\"Learning Rate: {params['learning_rate']:.4f}\")\n",
    "    print(f\"Number of Epochs: {round(params['num_epochs'])}\")\n",
    "    print(f\"Best Accuracy: {results['target']:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
